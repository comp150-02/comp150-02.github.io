---
layout: page
title: Final Project
permalink: /project/
---


<div class="panel-body">
                These project guidelines originally accompany the Stanford CS class <a href="http://cs231n.stanford.edu/">CS231n</a>, and are now provided here
                 with minor changes reflecting our course contents. Many thanks to Fei-Fei Li and Andrej Karpathy for graciously letting us use their course materials!
            </div>


  <h2>Final Project Demo Schedule</h2>
<ul>
<li>12:00 - 12:20 pm: Amit Patel - Deep Reinforcement Learning for Dota 2
<li>12:20 - 12:40 pm: Dylan, Nate, and Sam B. - Network Architectures for Multi-Label Video Tagging
<li>12:40 - 1:00 pm: Team Mango (Beibei, Cole, Hogyan) - RNN for 2.5D CNN
<li>1:00 - 1:20 pm: Jason, Lisa, and Sam - Sketch to Photo Translation
<li>1:20 - 1:40 pm: Jason, Jay, and Alex - Lung Cancer Detection
<li>1:40 - 2:00 pm: Justin, Jon, and Ben - Deep 3D+
<li>2:00 - 2:20 pm: Jie, Jorge, and Chris
<li>2:20 - 2:40 pm: Hossien and Xinmeng
<li>2:40 - 3:00 pm: Takuto

</ul>
  <h2>Important Dates</h2>

  Course project proposal: March 2 (due 11:59pm, email to comp150dl@gmail.com).<br>
  Course project milestone: April 4 (due 11:59pm, email to comp150dl@gmail.com).<br>
  Final project presentations will be held from 12pm-3pm on Friday May 12 at Microsoft New England Research and Development (NERD), 1 Memorial Drive 1st Floor, Cambridge, MA. Adams/Attucks Conference Room.<br>
  Final project write-up: <b>Friday May 12</b> (due 11:59pm, email to comp150dl@gmail.com).<br>

  <h2>Groups</h2>
<p>
These are the suggested teams for the semester project. If scheduling or other significant concerns arise, please email comp150dl@gmail.com. Teams will receive an email from the intructor with recommended project topics and research references.  
</p>
<ul>
<li>Team 1: Jason Krone,    Lisa Fan,    Sam Woolf
<li>Team 2: Alex Tong,    Jay DeStories,    Jason Fan
<li>Team 3: Beibei Du,    Hogyan Wang,    Cole Springate-Combs
<li>Team 4: Nathan Watts,    Dylan Cashman,    Sam Bruck
<li>Team 5: Ben Papp,    Justin Lee,    Jonathan Hohrath
<li>Team 6: Mohammad Hossein Chaghazardi,    Xinmeng Li,    Sambit Pradhan
<li>Team 7: Jie Li,    Jorge Sendino Lopez de la Reina, Chris Mattoli
<li>Team 8: Amit Patel
<li>Team 9: Takato Sato

</ul>

  <h2>Overview</h2>
  <p>The Final Project is an opportunity for you to apply what you have learned in class to a problem of your interest. Your are encouraged to select a topic with your group and work on your own project. Potential projects usually fall into these two tracks:</p>
    <ul>
      <li><strong>Applications.</strong> If you're coming to the class with a specific background and interests (e.g. biology, engineering, physics), we'd love to see you apply deep neural networks to problems related to your particular domain of interest. Pick a real-world problem and apply deep neural networks to solve it. </li>
      <li><strong>Models.</strong> You can build a new model (algorithm) with deep neural networks, or a new variant of existing models, and apply it to tackle vision tasks. This track might be more challenging, and sometimes leads to a piece of publishable work.</li>
    </ul>

    <p>Here you can find some sample project ideas:</p>
    <ul>
        <li>Any well known vision challenge such as COCO Detection or Keypoint Estimation, Visual Question Answering, or a Kaggle competition</li>
        <li>Describe images with Natural Language (COCO Attributes or Visual Genome Dataset)</li>
        <li><a href="https://www.yelp.com/dataset_challenge">Yelp Dataset Challenge</a>: describe restaurants from their Yelp images</li>
        <li>Dating artifacts with deep learning (use dataset collected from Spring 2016 version of this class)</li>
        <li>Deep Humor: generate funny captions for images (dataset collection required, contact instructor)</li>
        <li>Breast Cancer Prediction from Mammograms (contact instructor for dataset)</li>
        <li>Breast Cancer Prediction from Mammograms: Multiple Instance Learning (contact instructor for dataset)</li>
        <li>Rules for Social Media: characterize social network behavior of trans teens on tumblr  (contact instructor for dataset)</li>
        <li>Geolocation: predict a photo's GPS corrdinates (several datasets available)</li>
        <li>Segment reflective surfaces with <a href="https://hciweb.iwr.uni-heidelberg.de/hci/softwares/light_field_analysis">Light Field Data</a></li>
        <li>Recyclables recognition -- build your own dataset for this!</li>
        <li>Butterfly species recognition (ask instructor who to contact for this data)</li>
        <li>High fashion understanding using repository of New York Times fashion week images from last 20 years (instructor has this dataset)</li>
        <li>Alternate update scheme for faster training of Adversarial Nets (talk to instructor for more details)</li>
        <li>New loss function that optimizes for better generalization of network solution over lowest possible loss (talk to instructor for more details)</li>
        <li><a href="https://docs.google.com/document/d/1V_TYxxguUD_POT9PT3o5h4gnSecWHnQbfr-YpVy3V3E/edit?usp=sharing">Sample project ideas from UMass Amherst (Google Docs)</a></li>
    </ul>

    <p>To inspire ideas, you might look at the <a href="/papers/">papers read or recommended in this class</a>, recent deep learning publications from top-tier vision conferences, as well as other resources below.</p>
    <ul>
      <li><a href="https://github.com/kjw0612/awesome-deep-vision">Awesome Deep Vision</a></li>
      <li><a href="http://www.cv-foundation.org/openaccess/CVPR2016.py">CVPR</a>: IEEE Conference on Computer Vision and Pattern Recognition</li>
      <li><a href="http://www.cv-foundation.org/openaccess/ICCV2015.py">ICCV</a>: International Conference on Computer Vision</li>
      <li><a href="http://www.eccv2016.org/main-conference/">ECCV</a>: European Conference on Computer Vision</li>
      <li><a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015">NIPS</a>: Neural Information Processing Systems</li>
      <li><a href="http://nuit-blanche.blogspot.com/2016/02/iclr-2016-list-of-accepted-papers.html">ICLR</a>: International Conference on Learning Representations</li>
      <li><a href="http://www.kaggle.com/">Kaggle challenges</a>: An online machine learning competition website. For example, a <a href="https://www.kaggle.com/c/yelp-restaurant-photo-classification">Yelp classification challenge</a>.</li>
    </ul>
    <p>For applications, this type of projects would involve careful data preparation, an appropriate loss function, details of training and cross-validation and good test set evaluations and model comparisons. Don't be afraid to be creative. Some successful examples can be found below:</p>
      <ul>
        <li><a href="http://arxiv.org/abs/1412.3409">Teaching Deep Convolutional Neural Networks to Play Go</a></li>
        <li><a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
        <li><a href="http://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets/">Winning the Galaxy Challenge with convnets</a>
      </li></ul>
      Deep neural networks also run in real time on mobile phones and Raspberry Pi's - feel free to go the embedded way. You may find <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android">this TensorFlow demo on Android</a> helpful. </p>
    <p>For models, deep neural networks have been successfully used in a variety of computer vision and NLP tasks. This type of projects would involve understanding the state-of-the-art vision or NLP models, and building new models or improving existing models. The list below presents some papers on recent advances of deep neural networks in the computer vision community.</p>
    <ul>
      <li><strong>Object recognition</strong>: <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">[Krizhevsky et al.]</a>, <a href="http://arxiv.org/abs/1409.0575">[Russakovsky et al.]</a>, <a href="http://arxiv.org/abs/1409.4842">[Szegedy et al.]</a>, <a href="http://arxiv.org/abs/1409.1556">[Simonyan et al.]</a>, <a href="http://arxiv.org/abs/1406.4729">[He et al.]</a></li>
      <li><strong>Object detection</strong>: <a href="http://arxiv.org/abs/1311.2524">[Girshick et al.]</a>, <a href="http://arxiv.org/abs/1312.6229">[Sermanet et al.]</a>, <a href="http://arxiv.org/abs/1312.2249">[Erhan et al.]</a></li>
      <li><strong>Image segmentation</strong>: <a href="http://arxiv.org/abs/1411.4038">[Long et al.]</a></li>
      <li><strong>Video classification</strong>: <a href="http://cs.stanford.edu/people/karpathy/deepvideo/">[Karpathy et al.]</a>, <a href="http://arxiv.org/abs/1406.2199">[Simonyan and Zisserman]</a></li>
      <li><strong>Scene classification</strong>: <a href="http://places.csail.mit.edu/">[Zhou et al.]</a></li>
      <li><strong>Face recognition</strong>: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[Taigman et al.]</a></li>
      <li><strong>Depth estimation</strong>: <a href="http://www.cs.nyu.edu/~deigen/depth/">[Eigen et al.]</a></li>
      <li><strong>Image-to-sentence generation</strong>: <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">[Karpathy and Fei-Fei]</a>, <a href="http://arxiv.org/abs/1411.4389">[Donahue et al.]</a>, <a href="http://arxiv.org/abs/1411.4555">[Vinyals et al.]</a></li>
      <li><strong>Visualization and optimization</strong>: <a href="http://arxiv.org/pdf/1312.6199v4.pdf">[Szegedy et al.]</a>, <a href="http://arxiv.org/abs/1412.1897">[Nguyen et al.]</a>, <a href="http://arxiv.org/abs/1311.2901">[Zeiler and Fergus]</a>, <a href="http://arxiv.org/abs/1412.6572">[Goodfellow et al.]</a>, <a href="http://arxiv.org/abs/1312.6055">[Schaul et al.]</a></li>
    </ul>
    <p>We also provide a list of popular computer vision datasets:</p>
    <p>
      </p><ul>
        <li><a href="http://www.cvpapers.com/datasets.html">Meta Pointer: A large collection organized by CV Datasets.</a></li>
        <li><a href="http://riemenschneider.hayko.at/vision/dataset/">Yet another Meta pointer</a></li>
        <li><a href="http://http//image-net.org/">ImageNet</a>: a large-scale image dataset for visual recognition organized by <a href="http://wordnet.princeton.edu/">WordNet</a> hierarchy</li>
        <li><a href="http://groups.csail.mit.edu/vision/SUN/">SUN Database</a>: a benchmark for scene recognition and object detection with annotated scene categories and segmented objects</li>
        <li><a href="http://places.csail.mit.edu/">Places Database</a>: a scene-centric database with 205 scene categories and 2.5 millions of labelled images</li>
        <li><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth Dataset v2</a>: a RGB-D dataset of segmented indoor scenes</li>
        <li><a href="http://mscoco.org/">Microsoft COCO</a>: a new benchmark for image recognition, segmentation and captioning</li>
        <li><a href="http://yahoolabs.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images">Flickr100M</a>: 100 million creative commons Flickr images</li>
        <li><a href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild</a>: a dataset of 13,000 labeled face photographs</li>
        <li><a href="http://human-pose.mpi-inf.mpg.de/">Human Pose Dataset</a>: a benchmark for articulated human pose estimation</li>
        <li><a href="http://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces DB</a>: a face video dataset for unconstrained face recognition in videos</li>
        <li><a href="http://crcv.ucf.edu/data/UCF101.php">UCF101</a>: an action recognition data set of realistic action videos with 101 action categories</li>
        <li><a href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB-51</a>: a large human motion dataset of 51 action classes</li>
      </ul>
    <p></p>

  <h2>Grading Policy</h2>
  <pre style="background: bisque">  Final Project: 40%
  milestone: 5%
  write-up: 10%
   •  clarity, structure, language, references: 3%
   •  background literature survey, good understanding of the problem: 3%
   •  good insights and discussions of methodology, analysis, results, etc.: 4%
  technical: 12%
   •  correctness: 4%
   •  depth: 4%
   •  innovation: 4%
  evaluation and results: 10%
   •  sound evaluation metric: 3%
   •  thoroughness in analysis and experimentation: 3%
   •  results and performance: 4%
  presentation: 3% (+2% bonus for best few presentations)</pre>

  <h2>Project Proposal</h2>
  The project proposal should be one paragraph (200-400 words). Your proposal should contain:
  <p>
  </p><ul>
    <li>
    Who are the (1~3) group members? What will each person do? (This need to be a separate detailed paragraph)
    </li>
    <li>
    What is the problem that you will be investigating? Why is it interesting?
    </li>
    <li>
    What data will you use? If you are collecting new datasets, how do you plan to collect them?
    </li>
    <li>
    What method or algorithm are you proposing? If there are existing implementations, will you use them and how? How do you plan to improve or modify such implementations?
    </li>
    <li>
    What reading will you examine to provide context and background?
    </li>
    <li>
    How will you evaluate your results? Qualitatively, what kind of results do you expect (e.g. plots or figures)? Quantitatively, what kind of analysis will you use to evaluate and/or compare your results (e.g. what performance metrics or statistical tests)?
    </li>
  </ul>
<p>Each group should submit a plain-text proposal to comp150dl@gmail.com. If your proposed project is joint with
another class' project (with the consent of the other class' instructor), make this clear in the proposal.</p>



  <h2>Project Milestone</h2>
  Your project milestone report should be between 2 - 3 pages using the <a href="http://www.pamitc.org/cvpr15/files/cvpr2015AuthorKit.zip">provided template</a>. The following is a suggested structure for your report:
  <p>
    </p><ul>
      <li>Title, Author(s)</li>
      <li>Introduction: this section introduces your problem, and the overall plan for approaching your problem</li>
      <li>Problem statement: Describe your problem precisely specifying the dataset to be used, expected results and evaluation</li>
      <li>Technical Approach: Describe the methods you intend to apply to solve the given problem</li>
      <li>
      Intermediate/Preliminary Results: State and evaluate your results upto the milestone
      </li>
    </ul>
  <p></p>
  <p>
    <strong>Submission</strong>: Please email a PDF file named <code>&lt;your ID&gt;_milestone.pdf</code> to comp150dl@gmail.com. One submission for each group is sufficient.
  </p>

  <h2>Final Submission</h2>
<p>
  Your final write-up should be between <b>4 - 8</b> pages using the <a href="http://www.pamitc.org/cvpr15/files/cvpr2015AuthorKit.zip">provided template</a>. After the class, we will post all the final reports online so that you can read about each others' work. If you do not want your writeup to be posted online, then please let us know at least a week in advance of the final writeup submission deadline.
</p>

  <p>
  You will submit one or two files:
  </p>
  <ol>
    <li>A pdf file of your final report</li>
    <li>(OPTIONAL) zip file (or pdf file) with Supplementary Materials</li>
  </ol>

  <b>Report</b>. The following is a suggested structure for the report:
  <ul>
  <li>Title, Author(s)</li>
  <li>Abstract: It should not be more than 300 words</li>
  <li>Introduction: this section introduces your problem, and the overall plan for approaching your problem</li>
  <li>Background/Related Work: This section discusses relevant literature for your project</li>
  <li>Approach: This section details the framework of your project. Be specific, which means you might want to include equations, figures, plots, etc</li>
  <li>Experiment: This section begins with what kind of experiments you're doing, what kind of dataset(s) you're using, and what is the way you measure or evaluate your results. It then shows in details the results of your experiments. By details, we mean both quantitative evaluations (show numbers, figures, tables, etc) as well as qualitative results (show images, example results, etc).</li>
  <li>Conclusion: What have you learned? Suggest future ideas.</li>
  <li>References: This is absolutely necessary.</li>
  </ul>

  <b>Supplementary Material</b> is not counted toward your 4-8 page limit.

  <br>Examples of things to put in your supplementary material:
  <ul>
    <li>Source code (if your project proposed an algorithm, or code that is relevant and important for your project.).</li>
    <li>Cool videos, interactive visualizations, demos, etc.</li>
  </ul>
  Examples of things to not put in your supplementary material:
  <ul>
    <li>All of Caffe source code.</li>
    <li>Various ordinary data preprocessing scripts.</li>
    <li>Any code that is larger than 1MB.</li>
    <li>Model checkpoints.</li>
    <li>A computer virus.</li>
  </ul>

  <h2>Final Presentation</h2>
<p>
The culmination of this course will be Demo Day on May 12 from 12pm-3pm at Microsoft New England Research and Development (NERD), 1 Memorial Drive 1st Floor, Cambridge, MA. Adams/Attucks Conference Room. Each team will present their results of their project in a talk not exceeding 20 mins. Please leave 2-3 mins for questions. Teams that go over 20 mins will be given no mercy and will be hauled off stage. For examples of good talks, look for videos of oral presentations at NIPS, CVPR, ICCV, etc.
</p>


  <h2>Honor Code</h2>
<p>
  You may consult any papers, books, online references, or publicly
  available implementations for ideas and code that you may want to
  incorporate into your strategy or algorithm, so long as you clearly
  cite your sources in your code and your writeup. However, under no
  circumstances may you look at another group’s code or incorporate
  their code into your project.
</p>
<p>
  If you are doing a similar project for another class, you must make this clear and write down the exact portion of the project that is being counted for COMP 150DL.
</p>
    </div>

<h2>Previous Years' Projects</h2>
<p><strong>Brown Data-Driven Vision Final Projects (May 10, 2016)</strong></p>
  <p><a href="http://cs.brown.edu/courses/csci2951-t/finals/gmarques/">Rapid content based image retrieval</a> by Gustave Marques Netto</p>
  <p><a href="http://cs.brown.edu/courses/csci2951-t/finals/ghope/">Deep Learning for Natural Image Segmentation Priors</a> by Gabe Hope</p>
  <p><a href="http://cs.brown.edu/courses/csci2951-t/finals/cwhalen/">Determining artifact date and culture from images</a> by Christine Whalen</p>
<p><strong><a href="http://cs231n.stanford.edu/reports.html">CS231n Winter Quarter 2015 Final Projects</a></strong></p>
<p><strong><a href="https://docs.google.com/spreadsheets/d/10rbnBZGXzfDGnTRM8h4Q660ZZ731RSQosZr-RqtlUhU/edit?usp=sharing">UMass compsci697l Final Projects</a></strong></p>
