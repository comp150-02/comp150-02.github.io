---
layout: page
title: Notes
permalink: /notes/
---

  <p>
  These notes accompany the original Stanford CS class <a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a>. Many thanks to Fei-Fei Li and Andrej Karpathy for letting us use their course materials! 
  </p>

  <p>
  Additional supplementary notes provided by Erik Learned-Miller and Hang Su from the class <a href="https://compsci697l.github.io/index.html">COMPSCI 697L: Deep Learning</a>. Thanks Erik and Hang!
  </p>

    <h2>Setup</h2>
    <p>
      Setup your machine for this class or get set up on Tufts' AWS machines.
      </p>
    <h2>Module 0: Preparation</h2>

    <h4>
      <a href="https://compsci697l.github.io/notes/python-numpy-tutorial/">
        Python / Numpy Tutorial
      </a>
    </h4>

    <h4>
      <a href="https://compsci697l.github.io/notes/jupyter-tutorial/">
        Jupyter Notebook Tutorial
      </a>
    </h4>

    <h4>
      <a href="/notes/aws-tutorial/">
        Tufts AWS Tutorial
      </a>
    </h4>

    <h2>Module 0.5: Vector Calculus</h2>
    <h4>
      <a href="https://compsci697l.github.io/docs/vecDerivs.pdf">
        Vector, Matrix, and Tensor Derivatives
        </a>
      <h4>

    <h2>Module 1: Neural Networks</h4>

    <h4>
      <a href="https://compsci697l.github.io/notes/classification/">
        Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits
      </a>    </h4>
      <p>
        L1/L2 distances, hyperparameter search, cross-validation
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/linear-classify/">
         Linear classification: Support Vector Machine, Softmax
      </a>     </h4>
      <p>
        parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/optimization-1/">
        Optimization: Stochastic Gradient Descent
      </a>    </h4>
      <p>
        optimization landscapes, local search, learning rate, analytic/numerical gradient
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/optimization-2/">
        Backpropagation, Intuitions
      </a>    </h4>
      <p>
        chain rule interpretation, real-valued circuits, patterns in gradient flow
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/neural-networks-1/">
          Neural Networks Part 1: Setting up the Architecture
      </a>    </h4>
      <p>
        model of a biological neuron, activation functions, neural net architecture, representational power
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/neural-networks-2/">
          Neural Networks Part 2: Setting up the Data and the Loss
      </a>    </h4>
      <p>
          preprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/neural-networks-3/">
        Neural Networks Part 3: Learning and Evaluation
      </a>    </h4>
      <p>
        gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods, Adagrad/RMSprop, hyperparameter optimization, model ensembles
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/neural-networks-case-study/">
          Putting it together: Minimal Neural Network Case Study
      </a>    </h4>
      <p>
        minimal 2D toy data example
      </p>


    <h2>Module 2: Convolutional Neural Networks</h2>

    <h4>
      <a href="https://compsci697l.github.io/notes/convolutional-networks/">
        Convolutional Neural Networks: Architectures, Convolution / Pooling Layers
      </a>    </h4>
      <p>
          layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/understanding-cnn/">
        Understanding and Visualizing Convolutional Neural Networks
      </a>    </h4>
      <p>
        tSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons
      </p>


    <h4>
      <a href="https://compsci697l.github.io/notes/transfer-learning/">
        Transfer Learning and Fine-tuning Convolutional Neural Networks
      </a>
    </h4>
 


