---
layout: page
title: Papers
permalink: /papers/
---
<h2>Paper Presentations</h2>
  <article class="post-content">
  <p>In this assignment you will examine recent progress from the deep
  learning and computer vision literature. You will lead a class
  discussion about your assigned paper in the format of a graduate
  seminar or reading group. The goals of this assignment are</p>

<ul>
  <li>Practice reading and reviewing academic papers</li>
  <li>Learn to identify novel concepts</li>
  <li>Practice critical evaluation of research in Deep Learning</li>
  <li>Learn cannonical experimental frameworks and common metrics</li>
  <li>Lead an interesting discussion</li>
  <li>Practice using or re-implementing systems described in an academic paper</li>
</ul>

<h3>Q1: Summarize the Paper (25 points)</h3>
<p>Your presentation should cover all the major contributions of the assigned paper. Comparisons should be made to related works.</p>

<h3>Q2: Explain Key Contributions (20 points)</h3>
<p>Tell the class what is special about this paper compared to related work. Examine why the paper was well-cited or if you anticipate that it will be.</p>

<h3>Q3: Explain Weaknesses (20 points)</h3>
<p>Point out difficulties the paper's authors experienced or thing that you think the paper didn't address.</p>

<h3>Q4: Address in-class and blog comment questions (25 points)</h3>
<p>Respond to questions asked by your classmates.</p>

<h3>Q5: Demo something from the paper (10 points)</h3>
<p>Most papers will have a project page or some demo of the system presented in the paper. Show the class this demo and explain what's going on. If possible run the demo on new images or for new use cases not show in the paper.</p>

<h3 >Q6: Do something extra! (up to +10 points)</h3>
<p>You should feel free to implement
any part of the paper that you find interesting. This is a chance to combine your paper presentation with your final project. Implement some aspect of the paper that contributes to your own research. If the paper author's have released code or a pretrained model, try running that on the data you will use in your final project as a performance comparison or to use as a baseline.</p>


  </article>

<h2>Suggested Topics</h2>

<table class="table">


  <thead><tr>

    <td >Title, Authors</td>
    <td >Link to Paper, Project page</td>

  </tr></thead>
  <tr><td>
      <b>CVPR 2014 Tutorial on Deep Learning</b>. Graham Taylor, Marc'Aurelio Ranzato, and Honglak Lee.
      Read only the first two sets of labeled <a href="https://sites.google.com/site/deeplearningcvpr2014/DL-Intro-Lee.pdf">Introduction</a> and
      <a href="https://sites.google.com/site/deeplearningcvpr2014/ranzato_cvpr2014_DLtutorial.pdf">Supervised learning</a>.
    </td><td>
      <a href="https://sites.google.com/site/deeplearningcvpr2014/">CVPR 2014 tutorial</a>
    </td>
    </tr>  
      <tr><td>
      <b>ImageNet Classification with Deep Convolutional Neural Networks.</b> Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton. NIPS 2012.
    </td><td>
      <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">pdf</a>
    </td>
</tr>
  <tr>
    <td><b>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size</b>Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer, arXiv 2016.</td>
    <td><a href="https://arxiv.org/abs/1602.07360">arXiv</a></td>
  </tr>



<tr><td ><center>ConvNet detection and segmentation</center></td></tr>

</tr>
    <td><b>Object Detectors Emerge in Deep Scene CNNs.</b> Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba. ICLR, 2015.</td>
    <td><a href="http://places.csail.mit.edu/visualization/">project page</a>, <a href="http://arxiv.org/abs/1412.6856">arXiv</a></td>
    
</tr>  
<tr>
    
    <td><b>Learning Deep Features for Scene Recognition using Places Database.</b> B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. NIPS 2014.</td>
    <td><a href="http://places.csail.mit.edu/">project page</a>, <a href="http://places.csail.mit.edu/places_NIPS14.pdf">pdf</a>, <a href="http://places.csail.mit.edu/demo.html">demo</a></td>
    
</tr>



<tr>

    <td><b>DeepBox: Learning Objectness with Convolutional Networks.</b> Weicheng Kuo, Bharath Hariharan, Jitendra Malik. ICCV 2015.</td>
    <td><a href="http://arxiv.org/abs/1505.02146">arXiv</a></td>

</tr>

<tr>

    <td><b>Selective Search for Object Recognition.</b> J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, A. W. M. Smeulders. IJCV 2013.</td>
    <td><a href="https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&amp;bib=all.bib">project page</a></td>

</tr>


<tr>

    <td><b>Fast R-CNN.</b> Ross Girshick. ICCV 2015.</td>
    <td><a href="http://arxiv.org/abs/1504.08083">arXiv</a>, <a href="https://github.com/rbgirshick/fast-rcnn">code</a></td>

</tr>
<tr>
<td>
    <b>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.</b> Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. NIPS 2015.
</td><td><a href="http://arxiv.org/pdf/1506.01497v3.pdf">pdf</a> </td>
</tr>



<tr>

    <td><b>Fully Convolutional Networks for Semantic Segmentation.</b> Jonathan Long, Evan Shelhamer, Trevor Darrell. CVPR 2015.</td>
    <td><a href="http://arxiv.org/abs/1411.4038">arXiv</a></td>

</tr>



<tr>

    <td><b>Deep Neural Decision Forests.</b> Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulo. ICCV 2015.</td>
    <td><a href="http://research.microsoft.com/apps/pubs/?id=255952">Project page</a></td>

</tr>

  


<tr><td>
    <b>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.</b> R. Girshick, J. Donahue, T. Darrell, J. Malik. CVPR 2014.
</td><td>
    <a href="http://arxiv.org/abs/1311.2524">arXiv</a>
</td></tr>



<tr>

    <td><b>Going Deeper with Convolutions.</b> Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. 2014.</td>
    <td><a href="http://arxiv.org/abs/1409.4842">arXiv</a></td>

</tr>

<tr>
    <td><b>Diagnosing error in object detectors.</b> Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai. ECCV 2012.</td>
    <td><a href="http://web.engr.illinois.edu/~dhoiem/projects/detectionAnalysis/">project page</a></td>

</tr>

<tr><td><center> Vizualizing ConvNets</center> </td></tr>

<tr>

    <td><b>Understanding Deep Image Representations by Inverting Them.</b> Aravindh Mahendran, Andrea Vedaldi. CVPR 2015.</td>
    <td><a href="http://arxiv.org/abs/1412.0035">arXiv</a></td>
</tr>

<tr><td>
    <b>Visualizing and Understanding Convolutional Networks.</b> Matthew D Zeiler, Rob Fergus. ECCV 2014.
</td><td>
    <a href="http://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">pdf</a>
</td>
</tr>






  <tr><td ><center>Weakly Supervised and Unsupervised ConvNets</center></td></tr>

<tr>
    <td><b>Using very deep autoencoders for content-based image retrieval.</b>
Krizhevsky, Alex, and Geoffrey E. Hinton. ESANN. 2011.
    </td>
    <td><a href="http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf">pdf</a></td>

</tr>
  <tr>

    <td><b>Unsupervised Visual Representation Learning
	by Context Prediction.</b> Carl Doersch, Abhinav Gupta, Alexei A. Efros. ICCV 2015.</td>
    <td><a href="http://graphics.cs.cmu.edu/projects/deepContext/">project page</a></td>

  </tr>

  <tr>

    <td><b>Learning a Discriminative Model for the Perception of Realism in Composite Images.</b> Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, Alexei A. Efros. ICCV 2015.</td>
    <td><a href="http://www.eecs.berkeley.edu/~junyanz/projects/realism/index.html">project page</a></td>

  </tr>

  <tr>

    <td><b>Sketch-Based 3D Shape Retrieval Using Convolutional Neural Networks.</b> Fang Wang, Le Kang, Yi Li. CVPR 2015.</td>
    <td><a href="http://arxiv.org/abs/1504.03504">arXiv</a></td>

  </tr>

  <tr>

    <td><b>Multi-view Convolutional Neural Networks
	for 3D Shape Recognition.</b> Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller. ICCV 2015.</td>
    <td><a href="http://vis-www.cs.umass.edu/mvcnn/">project page</a></td>

  </tr>


<tr>
    <td><b>A High Performance CRF Model for Clothes Parsing.</b> E Simo-Serra,
S Fidler, F Moreno-Noguer, R Urtasun Computer Vision√êACCV 2014.
</td><td><a href="http://www.cs.toronto.edu/~urtasun/publications/simo_et_al_accv14.pdf">pdf</a>,
    <a href="https://github.com/bobbens/clothes_parsing">code</a></td>
</tr>

  <tr><td ><center>Siamese and Ranking ConvNets</center></td></tr>


<tr>

    <td><b>Learning Visual Similarity for Product Design with Convolutional Neural Networks.</b> Sean Bell, Kavita Bala. Siggraph 2015.</td>
    <td><a href="http://www.cs.cornell.edu/~sbell/">author page</a>, <a href="http://www.seanbell.ca/tmp/siggraph2015-bell-bala.pdf">pdf</a></td>

</tr>

  <tr>

    <td><b>Learning Deep Representations for Ground-to-Aerial Geolocalization.</b> Tsung-Yi Lin, Yin Cui, Serge Belongie, James Hays. CVPR 2015.</td>
    <td><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lin_Learning_Deep_Representations_2015_CVPR_paper.pdf">pdf</a></td>

  </tr>



  <tr>

    <td><b>Joint Embeddings of Shapes and Images via CNN Image Purification.</b>  Yangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. Siggraph Asia 2015.</td>
    <td><a href="https://shapenet.cs.stanford.edu/projects/JointEmbedding/">project page</a></td>

  </tr>





  <tr><td ><center>Images and Words</center></td></tr>

  <tr>

    <td><b>VISALOGY: Answering Visual Analogy Questions.</b> Fereshteh Sadeghi, C. Lawrence Zitnick, Ali Farhadi, NIPS 2015</td>
    <td><a href="https://arxiv.org/abs/1510.08973">arXiv</a></td>

  </tr>

  <tr>

    <td><b>Exploring Nearest Neighbor Approaches for Image Captioning.</b> Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C Lawrence Zitnick. arXiv, 2015.</td>
    <td><a href="http://arxiv.org/abs/1505.04467">arXiv</a></td>

  </tr>




  <tr>

    <td><b>Visual Madlibs: Fill in the blank Description Generation and Question Answering.</b> Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg. ICCV, 2015.</td>
    <td><a href="http://tamaraberg.com/visualmadlibs/">project page</a>, <a href="http://www.tamaraberg.com/papers/madlibs.pdf">pdf</a></td>

  </tr>

  
  <tr>

    <td><b>VQA: Visual Question Answering.</b> S. Antol*, A. Agrawal*, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. ICCV, 2015.</td>
    <td><a href="http://www.visualqa.org/">project page</a>, <a href="http://arxiv.org/abs/1505.00468">arXiv</a></td>

  </tr>

<tr>

    <td><b>Visual Turing test for computer vision systems.</b> Geman, Donald, et
al. Proceedings of the National Academy of Sciences 112.12 (2015):
3618-3623.</td>
    <td><a href="http://www.pnas.org/content/112/12/3618.abstract">PNAS page</a></td>

</tr>


  <tr><td ><center>Generative ConvNets</center></td></tr>

  <tr>

    <td><b>Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.</b> Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus. 2015.</td>
    <td><a href="http://soumith.ch/eyescream/">project page</a>, <a href="http://arxiv.org/abs/1506.05751">arXiv</a></td>

  </tr>

  <tr>

    <td><b>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.</b> Alec Radford, Luke Metz, Soumith Chintala. 2015.</td>
    <td><a href="https://github.com/Newmu/dcgan_code">project page</a>, <a href="http://arxiv.org/abs/1511.06434">arXiv</a></td>

  </tr>
  <tr>
    <td><b>Learning to Generate Chairs, Tables and Cars with Convolutional Networks.</b> Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko, Thomas Brox. CVPR 2015.</td>
    <td><a href="http://arxiv.org/abs/1411.5928">arXiv</a></td>

  </tr>

  <tr>

    <td><b>A Neural Algorithm of Artistic Style.</b> Leon A. Gatys, Alexander S. Ecker, Matthias Bethge. 2015.</td>
    <td><a href="https://github.com/jcjohnson/neural-style">implementation</a>, <a href="http://arxiv.org/abs/1508.06576">arXiv</a></td>

  </tr>
  <tr><td ><center>Co-Atention Networks</center></td></tr>
  <tr>

    <td><b>Hierarchical Question-Image Co-Attention for Visual Question Answering</b> Jiasen Lu, Jianwei Yang, Dhruv Batra , Devi Parikh, NIPS 2016.</td>
    <td><a href="https://arxiv.org/pdf/1606.00061v3.pdf">arXiv</a></td>

  </tr>


<tr>
  <td><center>Residual Networks</center></td>
</tr>

<tr>
  <td><b>Identity Mappings in Deep Residual Networks<b>Kaiming He , Xiangyu Zhang, Shaoqing Ren, Jian Sun, ECCV 2016</td>
  <td><a href="http://link.springer.com/chapter/10.1007/978-3-319-46493-0_38">paper</a></td>
</tr

<tr>
  <td><b>Residual networks behave like ensembles of relatively shallow networks<b>Andreas Veit, Michael J Wilber, Serge Belongie, NIPS 2016</td>
  <td><a href="http://papers.nips.cc/paper/6556-residual-networks-behave-like-ensembles-of-relatively-shallow-networks">nips.cc</a></td>
</tr>


<tr><td>
    <center>Datasets </center>
    </td>

</tr>

<tr><td>
    <b>Microsoft COCO: Common Objects in Context.</b> Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. ECCV 2014.</td>
        <td><a href="http://mscoco.org/">project page</a>, <a href="http://arxiv.org/abs/1405.0312">paper</a></td>

</tr>
    


<tr><td><b>The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding.</b> Genevieve Patterson, Chen Xu, Hang Su, James Hays. IJCV 2014. </td>
  <td><a href="http://cs.brown.edu/~gen/sunattributes.html">project page</a></td>
  </tr>
     
<tr>
  


  




  <tr><td ><center>Attribute-based Representations</center></td></tr>


  <tr>

    <td><b>Learning Deep Representations of Fine-grained Visual Descriptions.</b> Scott Reed, Zeynep Akata, Bernt Schiele, Honglak Lee, CVPR 2016.</td>
    <td><a href="https://arxiv.org/abs/1605.05395">arXiv</a></td>

  </tr>



  <tr><td>
      <b>Automatic attribute discovery and characterization from noisy web
  data.</b> Berg, Tamara L., Alexander C. Berg, and Jonathan Shih. Computer
  VisionECCV 2010. Springer Berlin Heidelberg, 2010. 663-676.
  </td><td>
      <a href="http://w.tamaraberg.com/papers/attributediscovery.pdf">pdf</a></td>
  </tr>




  <tr>
    <td><b>Discovering the Spatial Extent of Relative Attributes.</b> Fanyi Xiao, Yong Jae Lee. ICCV 2015.</td>
    <td><a href="http://www.cs.ucdavis.edu/~yjlee/projects/iccv2015.pdf">pdf</a></td>
  </tr>
  
  



  <tr><td ><center>Misc</center></td></tr>
<tr><td>
    <b>How do humans sketch objects?</b> Mathias Eitz, James Hays, and Marc Alexa. Siggraph 2012.
</td><td>
    <a href="http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/">project page</a>
</td>
</tr>


    <tr><td><b>Transient Attributes for High-Level Understanding and Editing of Outdoor Scenes.</b> Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao,
      Chao Qian, James Hays. Siggraph 2014.</td>
    <td><a href="http://transattr.cs.brown.edu/">project page</a></td>
      </tr>

  
  <tr><td><b>Learning to predict where humans look.</b> T. Judd, K. Ehinger, F. Durand, and A. Torralba. IEEE International Conference on
      Computer Vision (ICCV), 2009.</td><td><a href="http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">project page</a></td>
  </tr>

  <tr><td>
      <b>What makes Paris look like Paris?</b> Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros. Siggraph 2012.
  </td><td>
      <a href="http://graphics.cs.cmu.edu/projects/whatMakesParis/">project page</a>
  </td>
    </tr>


  <tr>

    <td><b>Learning Visual Biases from Human Imagination.</b> Carl Vondrick, Hamed Pirsiavash, Aude Oliva, Antonio Torralba. NIPS 2015.</td>
    <td><a href="http://web.mit.edu/vondrick/imagination/">project page</a></td>

  </tr>


<tr><td><b>Sketch2Photo: Internet Image Montage.</b> ACM SIGGRAPH ASIA 2009, ACM Transactions on Graphics. Tao Chen, Ming-Ming Cheng, Ping
    Tan, Ariel Shamir, Shi-Min Hu.</td><td><a href="http://www.ece.nus.edu.sg/stfpage/eletp/Projects/Sketch2Photo/index.htm">project page</a></td>
</tr>
<tr>
    <td><b>Eulerian video magnification for revealing subtle changes in the
world.</b> Wu, Hao-Yu, et al. ACM Trans. Graph. 31.4 (2012): 65.
    </td><td><a href="http://people.csail.mit.edu/mrub/evm/">project page</a></td>
    </tr>


  <tr><td><b>Photo tourism: Exploring photo collections in 3D.</b> Noah Snavely, Steven M. Seitz, Richard Szeliski. Siggraph 2006.</td><td><a href="http://phototour.cs.washington.edu/Photo_Tourism.pdf">pdf</a>, <a href="http://phototour.cs.washington.edu/">project page</a></td></tr>


</table>



